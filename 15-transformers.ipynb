{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Questions\n",
    "\n",
    "1. in self-attention, why do we use projection of K, Q, V instead of the origina values?\n",
    "\n",
    "In self-attention, if use the original values, K, Q, V will be the same all the time. Withou introducing learniable parameters, e.g., projections, the attention is always the same. Thus no attention can be learnt.\n",
    "By introducing the learnable parameters, self-attention module has the ability and flexibility to attend to different part of the sequence."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
