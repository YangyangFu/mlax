{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference\n",
    "\n",
    "The goal of generative models is to learn a distribution $p(x)$ over the data $x$. Variational inference is a method to approximate complex distributions by transforming them into simpler ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training a generative mode, the goal is to maximize the likelihood of the data, which can be expressed as:\n",
    "\n",
    "$$\\log p(x)$$\n",
    "\n",
    "Directly estimating $p(x)$ requires we have all the data in the world, which is not feasible. Based on chain rule, we can rewrite the log likelihood as:\n",
    "\n",
    "$$\\log p(x) = \\log \\int p(x, z) dz = \\log \\int p(x|z) p(z) dz$$\n",
    "\n",
    "\n",
    "However, this integral is intractable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Trick\n",
    "\n",
    "Now we introduce a **variational distribution** $q(z|x)$, an easier distribution to sample from. We can then rewrite the log likelihood as:\n",
    "\n",
    "$$\\log p(x) = E_{q(z|x)}[\\log p(x)]$$\n",
    "\n",
    "This holds because: $\\log p(x)$ is a constant with respect to $z$, independent of the integration variable. \n",
    "\n",
    "$$E_{q(z|x)}[\\log p(x)] = \\sum_z q(z|x) \\log p(x) = \\log p(x) \\sum_z q(z|x) = \\log p(x) \\times 1 = \\log p(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add and subtract $\\log q(z|x)$ inside the expectation:\n",
    "\n",
    "$$\\log p(x) = E_{q(z|x)}[\\log \\frac{p(x, z)}{q(z|x)} + \\log \\frac{q(z|x)}{p(z|x)}]$$\n",
    "\n",
    "This gives:\n",
    "\n",
    "$$ \\log p(x) = E_{q(z|x)}[\\log \\frac{p(x, z)}{q(z|x)}] + E_{q(z|x)}[\\log \\frac{q(z|x)}{p(z|x)}] $$\n",
    "\n",
    "\n",
    "The first term is the **variational evidence lower bound** (ELBO), which we want to maximize. \n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}(q) &= E_{q(z|x)}[\\log \\frac{p(x, z)}{q(z|x)}]\\\\\n",
    "    &= E_{q(z|x)}[\\log \\frac{p(x|z)p(z)}{q(z|x)}] \\\\\n",
    "    &= E_{q(z|x)}[\\log p(x|z)] + E_{q(z|x)}[\\log \\frac{p(z)}{q(z|x)}] \\\\\n",
    "    &= E_{q(z|x)}[\\log p(x|z)] - E_{q(z|x)}[\\log \\frac{q(z|x)}{p(z)}] \\\\\n",
    "    &= E_{q(z|x)}[\\log p(x|z)] - KL(q(z|x) || p(z))\n",
    "\n",
    "\\end{align*}\n",
    "\n",
    "The second term is the **KL divergence** between the variational distribution and the prior distribution:\n",
    "\n",
    "$$ KL(q(z|x) || p(z|x)) = E_{q(z|x)}[\\log \\frac{q(z|x)}{p(z|x)}] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that KL divergence is always non-negative**:\n",
    "\n",
    "Consider the function $f(x) = -\\log x$, which is convex, \n",
    "From Jensen's inequality, for a random variable $X$ and the convex function $f$, we have:\n",
    "$$ E[f(X)] \\geq f(E[X]) $$\n",
    "\n",
    "Apply $X = \\frac{p(z)}{q(z)}$:\n",
    "\n",
    "$$ E_{q(z)} [\\log(\\frac{p(z)}{q(z)})] \\leq \\log E_{q(z)}[\\frac{p(z)}{q(z)}] = \\log \\int_z q(z) \\frac{p(z)}{q(z)} dz = \\log 1 = 0 $$\n",
    "\n",
    "Thus, KL divergence is always non-negative:\n",
    "$$ KL(q(z) | p(z)) = E_{q(z)}[\\log \\frac{q(z)}{p(z)}] = - E_{q(z)} [\\log \\frac{p(z)}{q(z)}] \\geq 0 $$\n",
    "\n",
    "This means that maximizing the ELBO is equivalent to minimizing the KL divergence between the variational distribution and the prior distribution.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "With $KL >= 0$, maximizing the likelihood is equivalent to maximizing the ELBO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Gaussian mixture model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
