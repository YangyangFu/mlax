{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Networks\n",
    "\n",
    "```\n",
    "input -> [[CONV -> RELU] * N -> POOL?] * M -> [FC -> RELU]*K -> FC\n",
    "```\n",
    "\n",
    "where the * indicates repetition, and the POOL? indicates an optional pooling layer. Moreover, N >= 0 (and usually N <= 3), M >= 0, K >= 0 (and usually K < 3). For example, here are some common ConvNet architectures you may see that follow this pattern:\n",
    "\n",
    "see:\n",
    "\n",
    "- https://cs231n.github.io/convolutional-networks/#conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "\n",
    "Convolution leaverages three important ideas that help improve machine learning system:\n",
    "- sparse interactions\n",
    "- parameter sharing\n",
    "- equivariant representations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Sparse interactions\n",
    "- FCN layer uses matrix multiplication by a matrix of parameter with a seprate parameter describing the interaction between each input unit and each output unit. This means every output unit interacts with every input unit.\n",
    "- CNN have sparse interactions, by making the kernel smaller than the input. This leads to fewer parameters, which both reduces memory requirements of the model, improves its statistical efficiency and reduce operations.\n",
    "- for m inputs and n outputs, FCN will require $m \\times n$ parameters, and the algorithm have a $O(mn)$ runtime complexity, while CNN requires only $k \\times n$ parameters if we limit the number of conenctions each output may have to $k$. \n",
    "\n",
    "Parameter Sharing\n",
    "- In FCN, each element of the weight matrix is used exactly once when computing the output of a layer. In CNN, due to convolution, the same parameter in a kernel is used at every position of the input.\n",
    "\n",
    "Spatial Awareness\n",
    "- CNN preserve spatial relationship in data.\n",
    "- FCN treat 2-D image as 1-D vector, losing spatial info.\n",
    "\n",
    "Equivariant representation\n",
    "- CNN can detect features, e.g., edges, shapes regardless of their position in the image, thanks to shared filters and pooling operations\n",
    "- FCN doesn't have translation invariance since each input dimension is treated independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearity\n",
    "\n",
    "This stage add nonlinear activation to convolution results to study potential nonlinear relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "\n",
    "A pooling function replaces the output of a net at a certain location with a summary statistic of nearby points. For example, max pooling reports the maximum output within the neighborhood defined by the filter.\n",
    "\n",
    "- makes the representation invariant to small translation of the input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Questions\n",
    "\n",
    "1, Can You Please Describe the Structure of CNNs? the Different Layers, Activation Functions? What are Some Key Properties of Activation Functions?\n",
    "\n",
    "- Convolutional Neural Networks (CNNs) are a class of deep neural networks widely used in processing data with a grid-like topology, such as images. They are known for their ability to detect hierarchical patterns in data. Here’s an overview of their structure, including layers and activation functions:\n",
    "- Structure of CNNs\n",
    "    - Convolutional Layers: These layers apply a set of learnable filters (kernels) to the input. Each filter convolves across the width and height of the input volume, computing the dot product between the filter and input, producing a 2D activation map.\n",
    "      - Key Property: Convolutional layers are adept at capturing spatial hierarchies in images by learning from local regions (like edges, textures) in the early layers and more complex patterns (like objects, shapes) in deeper layers.\n",
    "    - Pooling Layers: Often placed after convolutional layers, pooling layers (such as max pooling or average pooling) reduce the spatial dimensions (width and height) of the input volume, leading to a reduction in the number of parameters and computation in the network. \n",
    "      - Key Property: Pooling helps in making the detection of features invariant to scale and orientation changes.\n",
    "    - Fully Connected Layers: At the end of the network, one or more fully connected layers are used where each neuron is connected to all neurons in the previous layer. These layers are typically used for classifying the features learned by the convolutional layers into different classes.\n",
    "      - Key Property: Fully connected layers combine features to make final predictions.\n",
    "  - Dropout: Dropout is a regularization technique used in CNNs to prevent overfitting. It randomly “drops” a subset of neurons in a layer during training, forcing the network to learn redundant representations and enhancing its generalization capabilities.\n",
    "  - Batch Normalization: Batch normalization is a technique to stabilize and accelerate the training of deep networks. It normalizes the activations of a previous layer at each batch, i.e., it applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n",
    "- Activation Functions\n",
    "  1. ReLU (Rectified Linear Unit):\n",
    "    Formula: $f(x)=\\max (0, x)$\n",
    "    Properties: Non-linear, allows models to account for complex data patterns; simple and efficient in computation.\n",
    "    Variants like Leaky ReLU or Parametric ReLU are used to address the “dying ReLU” problem where neurons can become inactive and stop contributing to the learning process.\n",
    "  2. Sigmoid:\n",
    "    Formula: $\\sigma(x)=\\frac{1}{1+e^{-x}}$\n",
    "    Properties: Smooth gradient, squashing values into a range between 0 and 1 . It’s often used in the output layer for binary classification.\n",
    "  3. Tanh (Hyperbolic Tangent):\n",
    "    Formula: $\\tanh (x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$\n",
    "    Properties: Similar to sigmoid but squashes values into a range between -1 and 1 . It is zerocentered, making it easier to model inputs that have strongly negative, neutral, and strongly positive values.\n",
    "  4. Softmax: Used in the output layer of a CNN for multi-class classification; it turns logits into probabilities that sum to one.\n",
    "    Properties: Softmax is non-linear and is able to handle multiple classes in a mutually exclusive scenario.\n",
    "- Key Properties of Activation Functions\n",
    "  - Nonlinearity: This allows CNNs to capture complex relationships in data. Without nonlinearity, the network would behave like a linear model.\n",
    "  - Differentiability: Essential for enabling backpropagation where gradients are computed during training.\n",
    "  - Computational Efficiency: Faster activation functions (like ReLU) lead to quicker training.\n",
    "\n",
    "In summary, the structure of CNNs, characterized by alternating convolutional and pooling layers followed by fully connected layers, combined with dropout for regularization and batch normalization for faster training, is optimized for feature detection and classification. The choice of activation function, critical for introducing nonlinearity, depends on the specific requirements of the task and the network architecture."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
